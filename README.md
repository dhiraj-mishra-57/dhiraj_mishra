# Overview:
AWS Data Engineer with 7+ years of proven track record in designing and implementing scalable real-time and batch data pipelines on AWS. Adept at optimising big data workflows and delivering actionable insights through robust analytics solutions. Architected and deployed data models supporting both OLAP and OLTP use cases for a major sports ticketing platform. Designed and implemented a data governance framework for PII encryption, and used AWS KMS (server-side and client-side) and secure data migration across environments. Expertise includes Spark performance tuning, large-scale data migrations, and leading cross-functional teams to enable data-driven decision-making.

## Technical Skills:
* Programming Languages: Python, Shell
* Cloud: AWS
* Processing & Streaming: Spark, MapReduce, Kafka
* Artificial Intelligence & LLMs: Amazon Bedrock, OpenAI (ChatGPT, DALL·E)
* Storage / DWH / Databases: Redshift, Oracle, Postgres, DynamoDB, Hadoop, Hive
* Data Orchestration: Step functions, Airflow
* AWS services: EMR, Glue, Glue catalogue, Athena, Lambda, S3, Kinesis Data Streams, DMS, IAM, EC2, SNS, ECS
* Data Integration/Management: Informatica (PowerCenter, BDM, MDM)
* Version control: Git, Jira, AWS CI/CD, Docker, Linux, Jira, SVN, Confluence


---

# Articles

## Real-time Streaming Guide: A Comprehensive Guide

Welcome to the **Real-time Streaming** series! This repository contains articles designed to help you master the streaming pipeline, from foundational concepts to advanced techniques. Whether you're a beginner or an experienced data engineer, these resources will guide you in understanding best practices in the streaming world.

#### 1. [Getting Started with Kinesis Data Stream](<https://medium.com/@dhirajmishra57/getting-started-with-kinesis-data-stream-1-29ef0ddd5b41>)
Learn the basics of Kinesis Data Streams and how to start building real-time data pipelines.

#### 2. [Materialized view use case in real-time streaming — Redshift](<https://medium.com/@dhirajmishra57/materialized-view-use-case-in-real-time-streaming-redshift-c21ed9ae444d>)
Understand how materialized views in Redshift can optimize real-time streaming analytics.

#### 3. [Streaming data from Postgres to Kinesis using Database Triggers](<https://medium.com/@dhirajmishra57/streaming-data-from-postgres-to-kinesis-using-database-triggers-7e74e5da74d5>)
Discover how to stream data from Postgres to Kinesis using database triggers for real-time processing.

---

## Spark Insights: A Comprehensive Guide

Welcome to the **Spark Insights** series! This repository contains articles designed to help you master Apache Spark, from foundational concepts to advanced techniques. Whether you're a beginner or an experienced data engineer, these resources will guide you in understanding Spark’s capabilities and best practices.

### Why Spark?

Apache Spark is a powerful and versatile tool for big data processing, providing unmatched speed, scalability, and flexibility. Its unified framework allows developers and analysts to easily process data at scale, making it an essential skill for anyone in the data ecosystem.

### Articles in the Series

#### 1. [Installing Spark 3.5.* on Windows](<https://medium.com/@dhirajmishra57/installing-spark-3-5-on-windows-e9bd183f84b9>)
Explore the fundamentals of Apache Spark, its architecture, and how it processes big data efficiently. This article is perfect for beginners looking to build a solid foundation in Spark.

#### 2. [Working with Date & Timestamp in PySpark](<https://medium.com/@dhirajmishra57/working-with-date-timestamp-in-pyspark-d590755fe806>)
Learn actionable tips and strategies to improve the performance of your Spark jobs. This article ensures your Spark workflows run faster and more efficiently, from configuration tuning to partitioning techniques.

#### 3. [Streaming data using Kafka, PostgreSQL, Spark Streaming, Airflow and Docker](<https://medium.com/@dhirajmishra57/streaming-data-using-kafka-postgresql-spark-streaming-airflow-and-docker-33c43bfa609d>)
Take your Spark expertise to the next level by mastering advanced PySpark techniques. Learn how to use window functions, handle large datasets, and design complex data pipelines.

---

## Data Science: Insights and Techniques

Welcome to the **Data Science** series, where we explore essential concepts and practical implementations to tackle real-world data challenges. These articles provide step-by-step guidance on handling complex data scenarios and implementing foundational techniques.

### Articles in the Series

#### 1. [Handling Outliers in Datasets](https://medium.com/@dhirajmishra57/handling-outliers-in-datasets-3740b04b52a2)
Learn various methods to identify and manage outliers in datasets, ensuring clean and reliable data for analysis.

#### 2. [Brief Overview of PCA and Implementation of Same Using Numpy](https://medium.com/swlh/brief-overview-of-pca-and-implementation-of-same-using-numpy-d864425e4a56)
Understand the concept of Principal Component Analysis (PCA) and how to implement it step-by-step using NumPy.

---
